{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590170c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export key tables for documentation and analysis\n",
    "\n",
    "export_dir = Path(\"diagnostic_exports\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Easy vs Hard exemplars\n",
    "easy_exemplars = pivot_models[pivot_models['difficulty_global'] == 'Easy'].nlargest(10, 'mean_mIoU')\n",
    "hard_exemplars = pivot_models[pivot_models['difficulty_global'] == 'Hard'].nsmallest(10, 'mean_mIoU')\n",
    "\n",
    "export_cols = ['mean_mIoU', 'std_mIoU', 'min_mIoU', 'max_mIoU', 'city', 'difficulty_global']\n",
    "easy_exemplars[export_cols].to_csv(export_dir / 'easy_exemplars.csv')\n",
    "hard_exemplars[export_cols].to_csv(export_dir / 'hard_exemplars.csv')\n",
    "\n",
    "print(\"✓ Exported exemplars:\")\n",
    "print(f\"  - easy_exemplars.csv ({len(easy_exemplars)} images)\")\n",
    "print(f\"  - hard_exemplars.csv ({len(hard_exemplars)} images)\")\n",
    "\n",
    "# 2. City statistics\n",
    "city_df.to_csv(export_dir / 'city_statistics.csv', index=False)\n",
    "print(f\"✓ Exported city_statistics.csv\")\n",
    "\n",
    "# 3. Class difficulty analysis\n",
    "class_difficulty_wide.to_csv(export_dir / 'class_difficulty_analysis.csv')\n",
    "print(f\"✓ Exported class_difficulty_analysis.csv\")\n",
    "\n",
    "# 4. Fingerprints\n",
    "fingerprints_df.to_csv(export_dir / 'fingerprints.csv', index=False)\n",
    "print(f\"✓ Exported fingerprints.csv\")\n",
    "\n",
    "# 5. Correlation summary\n",
    "correlations.to_csv(export_dir / 'feature_correlations.csv', header=['Correlation'])\n",
    "print(f\"✓ Exported feature_correlations.csv\")\n",
    "\n",
    "print(f\"\\nAll exports saved to: {export_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44714b58",
   "metadata": {},
   "source": [
    "## Section 9: Export Diagnostic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91722eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive diagnostic report\n",
    "print(\"=\" * 80)\n",
    "print(\"DIAGNOSTIC REPORT: Easy vs Hard Images Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. GLOBAL PATTERNS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total Images: {len(pivot_models)}\")\n",
    "print(f\"Easy Images: {(pivot_models['difficulty_global'] == 'Easy').sum()} ({(pivot_models['difficulty_global'] == 'Easy').sum() / len(pivot_models) * 100:.1f}%)\")\n",
    "print(f\"Hard Images: {(pivot_models['difficulty_global'] == 'Hard').sum()} ({(pivot_models['difficulty_global'] == 'Hard').sum() / len(pivot_models) * 100:.1f}%)\")\n",
    "print(f\"\\nPerformance Gap (Easy vs Hard):\")\n",
    "easy_perf = pivot_models[pivot_models['difficulty_global'] == 'Easy']['mean_mIoU'].mean()\n",
    "hard_perf = pivot_models[pivot_models['difficulty_global'] == 'Hard']['mean_mIoU'].mean()\n",
    "print(f\"  Easy: {easy_perf:.4f}\")\n",
    "print(f\"  Hard: {hard_perf:.4f}\")\n",
    "print(f\"  Gap: {easy_perf - hard_perf:.4f} ({(easy_perf - hard_perf) / hard_perf * 100:.1f}% difference)\")\n",
    "\n",
    "print(\"\\n2. STATISTICAL SIGNATURES\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Key Differentiators (based on effect size and statistical significance):\")\n",
    "print(f\"  - Model Agreement: Hard images show {fingerprints_df[fingerprints_df['Label']=='Hard']['Consensus'].values[0]:.4f} vs Easy {fingerprints_df[fingerprints_df['Label']=='Easy']['Consensus'].values[0]:.4f}\")\n",
    "print(f\"  - Uncertainty (Model Variance): Hard images show higher disagreement\")\n",
    "print(f\"  - Class Performance Variance: Hard images have uneven per-class performance\")\n",
    "\n",
    "print(\"\\n3. CITY-BASED PATTERNS\")\n",
    "print(\"-\" * 80)\n",
    "best_city = city_df.iloc[0]\n",
    "worst_city = city_df.iloc[-1]\n",
    "print(f\"Best performing city: {best_city['City']} (mIoU: {best_city['Mean mIoU']:.4f})\")\n",
    "print(f\"Worst performing city: {worst_city['City']} (mIoU: {worst_city['Mean mIoU']:.4f})\")\n",
    "print(f\"Easy/Hard Ratio by City (p-value={pval:.2e}):\")\n",
    "for _, row in city_df.head(5).iterrows():\n",
    "    print(f\"  {row['City']}: {row['Easy %']:.1f}% easy, {row['Hard %']:.1f}% hard\")\n",
    "\n",
    "print(\"\\n4. PER-CLASS INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "if 'Delta' in class_difficulty_wide.columns:\n",
    "    hardest_classes = class_difficulty_wide['Delta'].nsmallest(5)\n",
    "    easiest_classes = class_difficulty_wide['Delta'].nlargest(5)\n",
    "    \n",
    "    print(\"Classes most sensitive to image difficulty (largest Easy-Hard gap):\")\n",
    "    for cls, delta in easiest_classes.items():\n",
    "        print(f\"  {cls}: Δ={delta:.4f}\")\n",
    "    \n",
    "    print(\"\\nClasses least sensitive (small Easy-Hard gap):\")\n",
    "    for cls, delta in hardest_classes.items():\n",
    "        print(f\"  {cls}: Δ={delta:.4f}\")\n",
    "\n",
    "print(\"\\n5. FEATURE CORRELATIONS\")\n",
    "print(\"-\" * 80)\n",
    "top_positive = correlations.drop('target_mIoU').drop('target_easy').nlargest(3)\n",
    "top_negative = correlations.drop('target_mIoU').drop('target_easy').nsmallest(3)\n",
    "\n",
    "print(\"Features most correlated with HIGH performance:\")\n",
    "for feat, corr in top_positive.items():\n",
    "    print(f\"  {feat}: r={corr:.4f}\")\n",
    "\n",
    "print(\"\\nFeatures most correlated with LOW performance (difficulty):\")\n",
    "for feat, corr in top_negative.items():\n",
    "    print(f\"  {feat}: r={corr:.4f}\")\n",
    "\n",
    "print(\"\\n6. ACTIONABLE INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"• Hard images benefit from: targeted data augmentation, class-specific focus\")\n",
    "print(\"• Cities with low performance should be prioritized for model improvement\")\n",
    "print(f\"• Model disagreement is a key signal of image difficulty (Δ={easy_perf - hard_perf:.4f})\")\n",
    "print(\"• Some classes show high sensitivity to image difficulty - could be augmentation targets\")\n",
    "print(\"• Class imbalance and spatial complexity appear correlated with difficulty\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0968abc",
   "metadata": {},
   "source": [
    "## Section 8: Diagnostic Report - Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    def diagnostic_dashboard():\n",
    "        \"\"\"Interactive dashboard for easy/hard image exploration.\"\"\"\n",
    "        \n",
    "        style = {'description_width': 'initial'}\n",
    "        \n",
    "        # Difficulty selector\n",
    "        difficulty_selector = widgets.RadioButtons(\n",
    "            options=['Easy', 'Medium', 'Hard'],\n",
    "            value='Easy',\n",
    "            description='Image Difficulty:',\n",
    "            style=style\n",
    "        )\n",
    "        \n",
    "        # Number of images to show\n",
    "        num_images_slider = widgets.IntSlider(\n",
    "            value=5, min=1, max=15, step=1,\n",
    "            description='Number to Show:',\n",
    "            style=style\n",
    "        )\n",
    "        \n",
    "        # Sort metric\n",
    "        sort_metric = widgets.Dropdown(\n",
    "            options=[('Mean mIoU', 'mean_mIoU'), \n",
    "                     ('Model Std Dev', 'std_mIoU'),\n",
    "                     ('Min mIoU', 'min_mIoU'),\n",
    "                     ('Uncertainty', 'uncertainty')],\n",
    "            value='mean_mIoU',\n",
    "            description='Sort By:',\n",
    "            style=style\n",
    "        )\n",
    "        \n",
    "        output = widgets.Output()\n",
    "        \n",
    "        def update_dashboard(*args):\n",
    "            diff = difficulty_selector.value\n",
    "            n = num_images_slider.value\n",
    "            metric = sort_metric.value\n",
    "            \n",
    "            # Filter\n",
    "            subset = pivot_models[pivot_models['difficulty_global'] == diff].copy()\n",
    "            \n",
    "            # Create uncertainty column\n",
    "            subset['uncertainty'] = subset['max_mIoU'] - subset['min_mIoU']\n",
    "            \n",
    "            # Sort\n",
    "            if metric == 'uncertainty':\n",
    "                sorted_subset = subset.sort_values('uncertainty', ascending=False).head(n)\n",
    "            else:\n",
    "                sorted_subset = subset.sort_values(metric, ascending=(metric == 'std_mIoU')).head(n)\n",
    "            \n",
    "            with output:\n",
    "                clear_output()\n",
    "                \n",
    "                print(f\"Top {n} {diff} Images (sorted by {sort_metric.label}):\\n\")\n",
    "                \n",
    "                # Display stats\n",
    "                display_cols = ['mean_mIoU', 'std_mIoU', 'min_mIoU', 'max_mIoU', 'num_easy', 'num_hard', 'city']\n",
    "                display_data = sorted_subset[display_cols].copy()\n",
    "                display_data.columns = ['Mean IoU', 'Model Std', 'Min IoU', 'Max IoU', '#Easy Classes', '#Hard Classes', 'City']\n",
    "                display_data = display_data.round(4)\n",
    "                \n",
    "                display(display_data)\n",
    "                \n",
    "                # Per-image detailed stats\n",
    "                print(\"\\n\\nDetailed Per-Image Analysis:\")\n",
    "                for idx, (img_id, row) in enumerate(sorted_subset.iterrows(), 1):\n",
    "                    print(f\"\\n{idx}. {img_id} (City: {row['city']})\")\n",
    "                    print(f\"   Global: mean={row['mean_mIoU']:.4f}, std={row['std_mIoU']:.4f}, range=[{row['min_mIoU']:.4f}, {row['max_mIoU']:.4f}]\")\n",
    "                    print(f\"   Classes: {int(row['num_easy'])} easy, {int(row['num_hard'])} hard\")\n",
    "                    \n",
    "                    # Show per-class performance for this image\n",
    "                    image_classes = df_raw[df_raw['image_id'] == img_id].drop_duplicates('image_id')[class_cols]\n",
    "                    if not image_classes.empty:\n",
    "                        # Get mean across models\n",
    "                        mean_by_class = image_classes.iloc[0].sort_values(ascending=True)\n",
    "                        worst_classes = mean_by_class.head(3)\n",
    "                        best_classes = mean_by_class.tail(3)\n",
    "                        \n",
    "                        if not worst_classes.empty:\n",
    "                            print(f\"   Worst performers: {', '.join([f'{c}={v:.3f}' for c, v in worst_classes.items()])}\")\n",
    "                        if not best_classes.empty:\n",
    "                            print(f\"   Best performers: {', '.join([f'{c}={v:.3f}' for c, v in best_classes.items()])}\")\n",
    "        \n",
    "        difficulty_selector.observe(update_dashboard, names='value')\n",
    "        num_images_slider.observe(update_dashboard, names='value')\n",
    "        sort_metric.observe(update_dashboard, names='value')\n",
    "        \n",
    "        controls = widgets.VBox([\n",
    "            widgets.HBox([difficulty_selector, sort_metric]),\n",
    "            num_images_slider\n",
    "        ])\n",
    "        \n",
    "        dashboard = widgets.VBox([controls, output])\n",
    "        display(dashboard)\n",
    "        update_dashboard()\n",
    "    \n",
    "    print(\"Initializing Diagnostic Dashboard...\")\n",
    "    diagnostic_dashboard()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ipywidgets not available. Skipping interactive dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8c9d2",
   "metadata": {},
   "source": [
    "## Section 7: Interactive Diagnosis Dashboard\n",
    "\n",
    "Explore easy vs hard images with detailed metrics and per-class breakdowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64005624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a per-class difficulty heatmap\n",
    "# Rows: classes, Columns: easy/medium/hard, Values: mean IoU\n",
    "\n",
    "class_difficulty_stats = []\n",
    "\n",
    "for class_name in class_cols[:20]:  # First 20 classes\n",
    "    if class_name in per_class_analysis:\n",
    "        class_data = per_class_analysis[class_name]\n",
    "        \n",
    "        for diff_level in ['Easy', 'Medium', 'Hard']:\n",
    "            mask = class_data['difficulty'] == diff_level\n",
    "            if mask.any():\n",
    "                mean_iou = class_data[mask]['mean_iou'].mean()\n",
    "            else:\n",
    "                mean_iou = np.nan\n",
    "            \n",
    "            class_difficulty_stats.append({\n",
    "                'Class': class_name,\n",
    "                'Difficulty': diff_level,\n",
    "                'Mean IoU': mean_iou,\n",
    "                'Count': mask.sum()\n",
    "            })\n",
    "\n",
    "class_difficulty_wide = pd.DataFrame(class_difficulty_stats).pivot(\n",
    "    index='Class', columns='Difficulty', values='Mean IoU'\n",
    ")[['Easy', 'Medium', 'Hard']]\n",
    "\n",
    "print(\"Per-Class Performance by Image Difficulty:\\n\")\n",
    "print(class_difficulty_wide)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Heatmap of mean IoU by class and difficulty\n",
    "sns.heatmap(class_difficulty_wide, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Mean IoU'}, ax=axes[0], vmin=0, vmax=1)\n",
    "axes[0].set_title('Mean IoU: Class x Image Difficulty')\n",
    "axes[0].set_ylabel('Class')\n",
    "axes[0].set_xlabel('Image Difficulty Level')\n",
    "\n",
    "# Difficulty delta: (Easy - Hard) showing which classes benefit most from easy images\n",
    "if 'Easy' in class_difficulty_wide.columns and 'Hard' in class_difficulty_wide.columns:\n",
    "    class_difficulty_wide['Delta'] = class_difficulty_wide['Easy'] - class_difficulty_wide['Hard']\n",
    "    class_difficulty_wide_sorted = class_difficulty_wide.sort_values('Delta', ascending=True)\n",
    "    \n",
    "    colors = ['red' if x < 0 else 'green' for x in class_difficulty_wide_sorted['Delta']]\n",
    "    axes[1].barh(range(len(class_difficulty_wide_sorted)), \n",
    "                 class_difficulty_wide_sorted['Delta'], color=colors, alpha=0.7)\n",
    "    axes[1].set_yticks(range(len(class_difficulty_wide_sorted)))\n",
    "    axes[1].set_yticklabels(class_difficulty_wide_sorted.index)\n",
    "    axes[1].set_xlabel('IoU Difference (Easy - Hard)')\n",
    "    axes[1].set_title('Class Sensitivity to Image Difficulty')\n",
    "    axes[1].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Per-class difficulty analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457be90",
   "metadata": {},
   "source": [
    "## Section 6: Per-Class Difficulty Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feature matrix and correlate with performance\n",
    "# Features: class-level statistics and complexity indicators\n",
    "\n",
    "feature_matrix = pd.DataFrame(index=pivot_models.index)\n",
    "\n",
    "# Class-based features\n",
    "for class_name in class_cols[:15]:  # First 15 classes\n",
    "    if class_name in per_class_analysis:\n",
    "        class_data = per_class_analysis[class_name]\n",
    "        # How much is this class represented in each image? (non-NaN value = class present)\n",
    "        class_presence = class_data.notna().astype(float)\n",
    "        feature_matrix[f'{class_name}_present'] = class_presence[class_data.index].values if len(class_data) > 0 else 0\n",
    "\n",
    "# Class-wise IoU statistics\n",
    "feature_matrix['class_iou_std'] = pivot_models[class_cols].std(axis=1)  # Variance across classes\n",
    "feature_matrix['class_iou_min'] = pivot_models[class_cols].min(axis=1)  # Worst class performance\n",
    "feature_matrix['class_iou_mean'] = pivot_models[class_cols].mean(axis=1)  # Average class perf\n",
    "\n",
    "# Model consensus features\n",
    "feature_matrix['model_agreement'] = 1 - (pivot_models['std_mIoU'] / pivot_models['mean_mIoU'].clip(lower=0.01))\n",
    "feature_matrix['model_std'] = pivot_models['std_mIoU']\n",
    "\n",
    "# Target variable\n",
    "feature_matrix['target_mIoU'] = pivot_models['mean_mIoU']\n",
    "feature_matrix['target_easy'] = (pivot_models['difficulty_global'] == 'Easy').astype(int)\n",
    "\n",
    "print(\"Feature Matrix Summary:\")\n",
    "print(feature_matrix.describe())\n",
    "\n",
    "# Correlation with mIoU\n",
    "correlations = feature_matrix.corr()['target_mIoU'].sort_values(ascending=False)\n",
    "print(\"\\n\\nTop Correlations with Performance (mIoU):\")\n",
    "print(correlations.head(15))\n",
    "print(\"\\nNegative Correlations (indicating difficulty):\")\n",
    "print(correlations.tail(10))\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "important_features = correlations.head(12).index.tolist() + correlations.tail(3).index.tolist()\n",
    "corr_matrix = feature_matrix[important_features].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            cbar_kws={'label': 'Correlation'}, ax=ax, vmin=-1, vmax=1)\n",
    "ax.set_title('Correlation Matrix: Key Features and Performance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Correlation analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03760ae9",
   "metadata": {},
   "source": [
    "## Section 5: Correlation Analysis - Image Properties and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-city and per-metadata patterns\n",
    "print(\"City-wise Analysis:\\n\")\n",
    "\n",
    "city_stats = []\n",
    "for city in pivot_models['city'].unique():\n",
    "    city_mask = pivot_models['city'] == city\n",
    "    subset = pivot_models[city_mask]\n",
    "    \n",
    "    easy_ratio = (subset['difficulty_global'] == 'Easy').sum() / len(subset)\n",
    "    hard_ratio = (subset['difficulty_global'] == 'Hard').sum() / len(subset)\n",
    "    \n",
    "    city_stats.append({\n",
    "        'City': city,\n",
    "        'Count': len(subset),\n",
    "        'Mean mIoU': subset['mean_mIoU'].mean(),\n",
    "        'Easy %': easy_ratio * 100,\n",
    "        'Hard %': hard_ratio * 100,\n",
    "        'Std mIoU': subset['std_mIoU'].mean()\n",
    "    })\n",
    "\n",
    "city_df = pd.DataFrame(city_stats).sort_values('Mean mIoU', ascending=False)\n",
    "print(city_df.to_string())\n",
    "\n",
    "# Chi-squared test: City vs Easy/Hard\n",
    "city_difficulty_crosstab = pd.crosstab(pivot_models['city'], pivot_models['difficulty_global'])\n",
    "chi2, pval, dof, expected = stats.chi2_contingency(city_difficulty_crosstab)\n",
    "\n",
    "print(f\"\\n\\nChi-squared Test (City vs Difficulty):\")\n",
    "print(f\"  Chi2: {chi2:.4f}, p-value: {pval:.2e}\")\n",
    "print(f\"  Contingency table:\\n{city_difficulty_crosstab}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stacked bar chart\n",
    "city_df_sorted = city_df.sort_values('Mean mIoU')\n",
    "x = np.arange(len(city_df_sorted))\n",
    "axes[0].bar(x, city_df_sorted['Easy %'], label='Easy %', color='green', alpha=0.7)\n",
    "axes[0].bar(x, city_df_sorted['Hard %'], bottom=city_df_sorted['Easy %'], label='Hard %', color='red', alpha=0.7)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(city_df_sorted['City'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Percentage')\n",
    "axes[0].set_title('Easy/Hard Distribution by City')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Mean mIoU by city\n",
    "axes[1].barh(city_df_sorted['City'], city_df_sorted['Mean mIoU'], color='steelblue')\n",
    "axes[1].set_xlabel('Mean mIoU')\n",
    "axes[1].set_title('Average Performance by City')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Metadata analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448734a",
   "metadata": {},
   "source": [
    "## Section 4: Metadata-Based Attribute Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistical fingerprints: Easy vs Hard\n",
    "def compute_statistical_fingerprint(df, condition_mask, label):\n",
    "    \"\"\"Compute statistical properties for a cohort.\"\"\"\n",
    "    subset = df[condition_mask]\n",
    "    \n",
    "    stats_dict = {\n",
    "        'Label': label,\n",
    "        'Count': len(subset),\n",
    "        'Mean mIoU': subset['mean_mIoU'].mean(),\n",
    "        'Std mIoU': subset['std_mIoU'].mean(),\n",
    "        'Mean Class Variance': subset[class_cols].std(axis=1).mean(),\n",
    "        'Mean Per-Model Variance': subset.iloc[:, :-8].std(axis=1).mean(),  # Variance across models\n",
    "        'Max mIoU': subset['max_mIoU'].mean(),\n",
    "        'Min mIoU': subset['min_mIoU'].mean(),\n",
    "        'Uncertainty (Max-Min)': (subset['max_mIoU'] - subset['min_mIoU']).mean(),\n",
    "        'Consensus': 1 - (subset['std_mIoU'] / subset['mean_mIoU']).mean(),  # Inverse CoV\n",
    "    }\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "# Compute fingerprints\n",
    "easy_mask = pivot_models['difficulty_global'] == 'Easy'\n",
    "medium_mask = pivot_models['difficulty_global'] == 'Medium'\n",
    "hard_mask = pivot_models['difficulty_global'] == 'Hard'\n",
    "\n",
    "fingerprints = []\n",
    "fingerprints.append(compute_statistical_fingerprint(pivot_models, easy_mask, 'Easy'))\n",
    "fingerprints.append(compute_statistical_fingerprint(pivot_models, medium_mask, 'Medium'))\n",
    "fingerprints.append(compute_statistical_fingerprint(pivot_models, hard_mask, 'Hard'))\n",
    "\n",
    "fingerprints_df = pd.DataFrame(fingerprints)\n",
    "print(\"Statistical Fingerprints of Easy vs Hard Images:\\n\")\n",
    "display(fingerprints_df.set_index('Label'))\n",
    "\n",
    "# Statistical tests: Easy vs Hard\n",
    "easy_mious = pivot_models[easy_mask]['mean_mIoU']\n",
    "hard_mious = pivot_models[hard_mask]['mean_mIoU']\n",
    "\n",
    "# T-test\n",
    "t_stat, t_pval = stats.ttest_ind(easy_mious, hard_mious)\n",
    "# Effect size (Cohen's d)\n",
    "cohens_d = (easy_mious.mean() - hard_mious.mean()) / np.sqrt(((len(easy_mious)-1)*easy_mious.std()**2 + \n",
    "                                                                   (len(hard_mious)-1)*hard_mious.std()**2) / \n",
    "                                                                  (len(easy_mious) + len(hard_mious) - 2))\n",
    "\n",
    "print(f\"\\n\\nStatistical Tests: Easy vs Hard\")\n",
    "print(f\"  T-statistic: {t_stat:.4f}, p-value: {t_pval:.2e}\")\n",
    "print(f\"  Cohen's d (effect size): {cohens_d:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Distribution\n",
    "axes[0, 0].hist(easy_mious, bins=20, alpha=0.6, label='Easy', color='green')\n",
    "axes[0, 0].hist(hard_mious, bins=20, alpha=0.6, label='Hard', color='red')\n",
    "axes[0, 0].set_xlabel('Mean mIoU')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution: Easy vs Hard')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot\n",
    "box_data = [easy_mious, hard_mious]\n",
    "axes[0, 1].boxplot(box_data, labels=['Easy', 'Hard'])\n",
    "axes[0, 1].set_ylabel('Mean mIoU')\n",
    "axes[0, 1].set_title('mIoU Distribution: Box Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Uncertainty\n",
    "easy_uncertainty = pivot_models[easy_mask]['max_mIoU'] - pivot_models[easy_mask]['min_mIoU']\n",
    "hard_uncertainty = pivot_models[hard_mask]['max_mIoU'] - pivot_models[hard_mask]['min_mIoU']\n",
    "\n",
    "axes[1, 0].scatter(pivot_models[easy_mask]['mean_mIoU'], easy_uncertainty, \n",
    "                   alpha=0.5, label='Easy', color='green', s=50)\n",
    "axes[1, 0].scatter(pivot_models[hard_mask]['mean_mIoU'], hard_uncertainty, \n",
    "                   alpha=0.5, label='Hard', color='red', s=50)\n",
    "axes[1, 0].set_xlabel('Mean mIoU')\n",
    "axes[1, 0].set_ylabel('Model Uncertainty (Max-Min mIoU)')\n",
    "axes[1, 0].set_title('Performance vs Uncertainty')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance across classes\n",
    "easy_class_var = pivot_models[easy_mask][class_cols].std(axis=1)\n",
    "hard_class_var = pivot_models[hard_mask][class_cols].std(axis=1)\n",
    "\n",
    "axes[1, 1].hist(easy_class_var, bins=20, alpha=0.6, label='Easy', color='green')\n",
    "axes[1, 1].hist(hard_class_var, bins=20, alpha=0.6, label='Hard', color='red')\n",
    "axes[1, 1].set_xlabel('Variance Across Classes')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Class-wise Performance Variance')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Statistical fingerprinting complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c717a",
   "metadata": {},
   "source": [
    "## Section 3: Statistical Fingerprinting of Easy vs Hard Images\n",
    "\n",
    "Compute descriptive statistics comparing easy vs hard image cohorts to identify statistically significant differentiators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overlaps: which images are easy for all classes vs. hard for specific classes\n",
    "print(\"Overlap Analysis: Easy/Hard Consistency Across Classes\\n\")\n",
    "\n",
    "# Collect easy/hard status per image per class\n",
    "class_difficulty_matrix = pd.DataFrame(index=pivot_models.index)\n",
    "\n",
    "for class_name in class_cols[:10]:  # First 10 classes\n",
    "    if class_name in per_class_analysis:\n",
    "        class_difficulty_matrix[class_name] = per_class_analysis[class_name]['difficulty']\n",
    "\n",
    "# Count how many classes are easy/hard for each image\n",
    "class_difficulty_matrix['num_easy'] = (class_difficulty_matrix == 'Easy').sum(axis=1)\n",
    "class_difficulty_matrix['num_hard'] = (class_difficulty_matrix == 'Hard').sum(axis=1)\n",
    "class_difficulty_matrix['num_classes'] = class_difficulty_matrix.iloc[:, :-2].count(axis=1)\n",
    "\n",
    "# Merge back to main dataframe\n",
    "pivot_models = pivot_models.join(class_difficulty_matrix[['num_easy', 'num_hard', 'num_classes']])\n",
    "\n",
    "print(\"How many classes are Easy/Hard per image?\")\n",
    "print(\"\\nEasy classes per image:\")\n",
    "print(pivot_models['num_easy'].describe())\n",
    "print(\"\\nHard classes per image:\")\n",
    "print(pivot_models['num_hard'].describe())\n",
    "\n",
    "# Identify images universally easy vs hard\n",
    "universally_easy = pivot_models[(pivot_models['difficulty_global'] == 'Easy') & \n",
    "                                (pivot_models['num_easy'] > 5)]\n",
    "universally_hard = pivot_models[(pivot_models['difficulty_global'] == 'Hard') & \n",
    "                                (pivot_models['num_hard'] > 5)]\n",
    "class_specific_hard = pivot_models[(pivot_models['num_hard'] > 3) & \n",
    "                                   (pivot_models['difficulty_global'] != 'Hard')]\n",
    "\n",
    "print(f\"\\nImages universally easy: {len(universally_easy)}\")\n",
    "print(f\"Images universally hard: {len(universally_hard)}\")\n",
    "print(f\"Images with class-specific hardness: {len(class_specific_hard)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(pivot_models['num_easy'], bins=15, alpha=0.6, label='Easy', color='green')\n",
    "axes[0].hist(pivot_models['num_hard'], bins=15, alpha=0.6, label='Hard', color='red')\n",
    "axes[0].set_xlabel('Number of Classes')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Easy/Hard Classes per Image')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(pivot_models['num_easy'], pivot_models['num_hard'], alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Easy Classes')\n",
    "axes[1].set_ylabel('Number of Hard Classes')\n",
    "axes[1].set_title('Easy vs Hard Classes: Scatter Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Overlap analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf89442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create easy/hard classifications based on percentiles\n",
    "EASY_THRESHOLD = 0.75  # 75th percentile = easy\n",
    "HARD_THRESHOLD = 0.25  # 25th percentile = hard\n",
    "\n",
    "pivot_models['difficulty_global'] = pd.cut(\n",
    "    pivot_models['mean_mIoU'],\n",
    "    bins=[0, pivot_models['mean_mIoU'].quantile(HARD_THRESHOLD),\n",
    "          pivot_models['mean_mIoU'].quantile(EASY_THRESHOLD), 1.0],\n",
    "    labels=['Hard', 'Medium', 'Easy'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print(\"Global Easy/Hard Distribution:\")\n",
    "print(pivot_models['difficulty_global'].value_counts().sort_index())\n",
    "print(f\"\\nEasy threshold (75th pct): {pivot_models['mean_mIoU'].quantile(EASY_THRESHOLD):.4f}\")\n",
    "print(f\"Hard threshold (25th pct): {pivot_models['mean_mIoU'].quantile(HARD_THRESHOLD):.4f}\")\n",
    "\n",
    "# Per-class analysis\n",
    "per_class_analysis = {}\n",
    "\n",
    "for class_name in class_cols[:5]:  # Start with first 5 for demonstration\n",
    "    class_data = df_raw[['image_id', 'model', class_name]].dropna()\n",
    "    \n",
    "    if class_data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Pivot: index=image_id, columns=models\n",
    "    class_pivot = class_data.pivot(index='image_id', columns='model', values=class_name)\n",
    "    class_pivot['mean_iou'] = class_pivot.mean(axis=1)\n",
    "    class_pivot['std_iou'] = class_pivot.std(axis=1)\n",
    "    \n",
    "    # Classify\n",
    "    class_pivot['difficulty'] = pd.cut(\n",
    "        class_pivot['mean_iou'],\n",
    "        bins=[0, class_pivot['mean_iou'].quantile(HARD_THRESHOLD),\n",
    "              class_pivot['mean_iou'].quantile(EASY_THRESHOLD), 1.0],\n",
    "        labels=['Hard', 'Medium', 'Easy'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    per_class_analysis[class_name] = class_pivot\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  Images: {len(class_pivot)}, Mean IoU: {class_pivot['mean_iou'].mean():.4f}\")\n",
    "    print(f\"  Distribution:\\n{class_pivot['difficulty'].value_counts().sort_index()}\")\n",
    "\n",
    "print(\"\\n✓ Per-class categorization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0284d",
   "metadata": {},
   "source": [
    "## Section 2: Define Easy/Hard Image Categories by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and calculate mIoU per image\n",
    "class_cols = [c for c in df_raw.columns if c not in ['image_id', 'city', 'model']]\n",
    "print(f\"Classes identified: {len(class_cols)} classes\")\n",
    "\n",
    "# Calculate per-image mIoU (mean across classes)\n",
    "df_raw['image_mIoU'] = df_raw[class_cols].mean(axis=1)\n",
    "\n",
    "# Pivot to get one row per image with models as columns\n",
    "pivot_models = df_raw.pivot(index='image_id', columns='model', values='image_mIoU')\n",
    "pivot_models['mean_mIoU'] = pivot_models.mean(axis=1)\n",
    "pivot_models['std_mIoU'] = pivot_models.std(axis=1)\n",
    "pivot_models['min_mIoU'] = pivot_models.min(axis=1)\n",
    "pivot_models['max_mIoU'] = pivot_models.max(axis=1)\n",
    "\n",
    "# Extract metadata from image_id\n",
    "pivot_models['city'] = pivot_models.index.str.split('_').str[0]\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Total images: {len(pivot_models)}\")\n",
    "print(f\"  Mean mIoU: {pivot_models['mean_mIoU'].mean():.4f} ± {pivot_models['mean_mIoU'].std():.4f}\")\n",
    "print(f\"  Range: [{pivot_models['mean_mIoU'].min():.4f}, {pivot_models['mean_mIoU'].max():.4f}]\")\n",
    "print(f\"  Cities: {pivot_models['city'].nunique()}\")\n",
    "\n",
    "display(pivot_models.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cb557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    CITYSCAPES_ROOT = Path(\"/content/drive/MyDrive/UCLA/Datasets/cityscapes\")\n",
    "except ImportError:\n",
    "    CITYSCAPES_ROOT = Path(\"data/cityscapes\")\n",
    "    print(\"Running locally (not Colab)\")\n",
    "\n",
    "RESULTS_DIR = CITYSCAPES_ROOT / \"benchmark_results\"\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "\n",
    "# Load benchmark results\n",
    "def load_benchmark_results(results_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load all *_per_image_iou.csv files.\"\"\"\n",
    "    all_files = list(results_dir.glob(\"*_per_image_iou.csv\"))\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"No result files found in {results_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dfs = []\n",
    "    for f in all_files:\n",
    "        model_name = f.name.replace(\"_per_image_iou.csv\", \"\").replace(\"Wrapper\", \"\")\n",
    "        df = pd.read_csv(f)\n",
    "        df[\"model\"] = model_name\n",
    "        dfs.append(df)\n",
    "        print(f\"✓ Loaded {len(df)} images for model: {model_name}\")\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "df_raw = load_benchmark_results(RESULTS_DIR)\n",
    "print(f\"\\nTotal records: {len(df_raw)}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()[:5]}...\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3c4cc",
   "metadata": {},
   "source": [
    "## Section 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29b21d",
   "metadata": {},
   "source": [
    "# Advanced Diagnostic Analysis: Easy vs Hard Images\n",
    "## Finding Associated Attributes of Image Difficulty\n",
    "\n",
    "This notebook extends the Cross_Model_Analysis work by conducting deep diagnostic research to identify **which attributes distinguish easy (universally well-segmented) from hard (universally poorly-segmented) images** across multiple segmentation models and per-category analysis.\n",
    "\n",
    "### Key Questions:\n",
    "1. **Statistical Signatures**: What statistical patterns differentiate easy vs hard images?\n",
    "2. **Metadata Patterns**: Are certain cities/seasons/conditions associated with difficulty?\n",
    "3. **Per-Class Dynamics**: Which classes are hard to segment in hard images?\n",
    "4. **Feature Correlation**: Which image properties predict segmentation difficulty?\n",
    "5. **Visual Patterns**: Can we identify visual patterns in easy vs hard cohorts?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
