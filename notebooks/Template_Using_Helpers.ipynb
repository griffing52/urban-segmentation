{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8a1929",
   "metadata": {},
   "source": [
    "# Template: Using Helper Functions in Your Model Notebook\n",
    "\n",
    "This notebook demonstrates how to integrate the helper utilities into your existing model evaluation notebooks.\n",
    "It shows a minimal example using the new `dataset_utils`, `model_utils`, and `analysis_utils` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044fb43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP: Import the helper utilities\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "src_path = Path(\"../src\").resolve()\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import utilities\n",
    "from dataset_utils import make_cityscapes_dataframe\n",
    "from model_utils import BaseSegmentationModel, run_inference_over_df, evaluate_model_on_split\n",
    "from analysis_utils import compute_image_statistics, identify_easy_vs_hard\n",
    "\n",
    "print(\"✓ Helpers imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Set up your model\n",
    "# Your model must inherit from BaseSegmentationModel and implement predict()\n",
    "\n",
    "from abc import abstractmethod\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class YourModelWrapper(BaseSegmentationModel):\n",
    "    \"\"\"Example wrapper for your model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"your-model-name\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Loading {model_name} on {self.device}...\")\n",
    "        # Load your model here\n",
    "        # self.model = ... \n",
    "        \n",
    "    def predict(self, image: Image.Image) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Run inference and return segmentation mask.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image in RGB mode\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray of shape (H, W) with integer trainIds (0-18)\n",
    "        \"\"\"\n",
    "        # Implement your inference logic here\n",
    "        # return pred_mask  # Should be np.ndarray with trainIds\n",
    "        pass\n",
    "\n",
    "# Instantiate\n",
    "model = YourModelWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e699300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Set up paths and load dataset\n",
    "CITYSCAPES_ROOT = Path(\"data/cityscapes\")  # Update path as needed\n",
    "\n",
    "# Load validation split into a DataFrame\n",
    "val_df = make_cityscapes_dataframe(CITYSCAPES_ROOT, split=\"val\")\n",
    "print(f\"Loaded {len(val_df)} validation images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Run inference (once per model)\n",
    "# Save predictions as .npy files\n",
    "\n",
    "model_name = \"YourModelWrapper\"\n",
    "pred_dir = CITYSCAPES_ROOT / f\"{model_name}_preds\" / \"val\"\n",
    "\n",
    "# This runs inference and saves predictions\n",
    "val_df_with_preds = run_inference_over_df(\n",
    "    val_df,\n",
    "    model=model,\n",
    "    pred_root=pred_dir,\n",
    "    overwrite=False  # Skip if already exists\n",
    ")\n",
    "\n",
    "print(\"✓ Inference complete\")\n",
    "print(val_df_with_preds[['image_id', 'pred_trainIds_path']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a92100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Evaluate and save results\n",
    "from model_utils import evaluate_model_on_split, save_results_csv\n",
    "\n",
    "# Set up ground truth and results directories\n",
    "gt_dir = CITYSCAPES_ROOT / \"gtFine_trainvaltest\" / \"gtFine\" / \"val\"\n",
    "results_dir = CITYSCAPES_ROOT / \"benchmark_results\"\n",
    "\n",
    "# Evaluate (computes IoU for each image/class)\n",
    "results_df = evaluate_model_on_split(\n",
    "    pred_dir=pred_dir,\n",
    "    gt_dir=gt_dir,\n",
    "    split_df=val_df,\n",
    "    model_name=model_name,\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_results_csv(results_df, results_dir, model_name)\n",
    "\n",
    "print(\"✓ Evaluation complete\")\n",
    "print(f\"Results shape: {results_df.shape}\")\n",
    "display(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Analyze results with helper functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute image statistics\n",
    "stats = compute_image_statistics(results_df)\n",
    "print(f\"Computed stats for {len(stats)} images\")\n",
    "\n",
    "# Identify easy and hard images\n",
    "easy, hard = identify_easy_vs_hard(stats, n_images=10)\n",
    "\n",
    "print(f\"\\nTop 5 EASIEST images:\")\n",
    "display(easy.head(5))\n",
    "\n",
    "print(f\"\\nTop 5 HARDEST images:\")\n",
    "display(hard.head(5))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(stats['mean_performance'], stats['moe_gain'], alpha=0.5)\n",
    "ax.set_xlabel('Mean Model Performance (mIoU)')\n",
    "ax.set_ylabel('MoE Potential Gain (max - mean)')\n",
    "ax.set_title('Image Difficulty vs MoE Opportunity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5755f90",
   "metadata": {},
   "source": [
    "## Quick Integration Tips\n",
    "\n",
    "1. **Inherit from `BaseSegmentationModel`**: Your model wrapper should implement the `predict(image: Image.Image) -> np.ndarray` method.\n",
    "\n",
    "2. **Use `run_inference_over_df()`**: Saves predictions to `.npy` files efficiently. Set `overwrite=False` to skip re-running.\n",
    "\n",
    "3. **Use `evaluate_model_on_split()`**: Computes per-image, per-class IoU automatically. Results are saved as CSV.\n",
    "\n",
    "4. **Use `compute_image_statistics()`**: Quickly identify easy vs hard images and MoE potential.\n",
    "\n",
    "5. **For Hard Subset Analysis**: Import `create_hard_subset()` from `dataset_utils` and use `compare_subsets()` from `dataset_utils` to evaluate on the hard subset separately.\n",
    "\n",
    "See `All_vs_Hard_Cityscapes.ipynb` for a complete example of hard subset evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
