{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455294ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "IMMEDIATE ACTION ITEMS (This Week)\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "PRIORITY 1 - Full Evaluation (3-4 hours)\n",
    "─────────────────────────────────────────────────────────────────────────────\n",
    "[ ] Run generation on full validation set (500 images)\n",
    "    Command: python scripts/generate_sam3_masks.py --data_root data/cityscapes --max_images -1\n",
    "    \n",
    "[ ] Evaluate all configs: Compute metrics for entire dataset\n",
    "    Create comprehensive results.csv with all 500×4 comparisons\n",
    "    \n",
    "[ ] Summary statistics: Which config is clearly best?\n",
    "    Does L2 Descriptive beat L1 Baseline everywhere?\n",
    "    Does multi-crop beat baseline?\n",
    "\n",
    "Expected output: \n",
    "  - results_full.csv (500 images × 4 configs)\n",
    "  - Summary showing ~0.50-0.70 IoU range (typical for boundaries)\n",
    "  - Best config clear winner\n",
    "\n",
    "PRIORITY 2 - Baseline Comparison (1-2 days)\n",
    "─────────────────────────────────────────────────────────────────────────────\n",
    "[ ] Get SegFormer baseline (no auxiliary loss)\n",
    "    - What's the baseline boundary IoU without SAM3 guidance?\n",
    "    - This becomes your reference point\n",
    "    \n",
    "[ ] Quick Mask2Former comparison if possible\n",
    "    - Pre-trained model on similar task\n",
    "    - How much does SAM3+prompts help?\n",
    "\n",
    "[ ] Create comparison table:\n",
    "    | Method          | Boundary IoU | Dice | F1 |\n",
    "    |─────────────────|────────────--|------|─---|\n",
    "    | SegFormer       | 0.52         | 0.65 |    |\n",
    "    | Mask2Former     | 0.54         | 0.66 |    |\n",
    "    | SAM3 Baseline   | 0.60         | 0.72 |    | ← best config\n",
    "    | SAM3 + L2       | 0.63         | 0.74 |    |\n",
    "\n",
    "PRIORITY 3 - Prompt Analysis (2-3 hours)\n",
    "─────────────────────────────────────────────────────────────────────────────\n",
    "[ ] Quantify the prompt effect:\n",
    "    L1→L2 improvement: +0.03 IoU (+5%)\n",
    "    L2→L3 improvement: +0.01 IoU (+1.5%)\n",
    "    Total: +0.04 IoU overall\n",
    "    \n",
    "[ ] Visualize what changes from L1→L2→L3\n",
    "    - What boundaries does L2 catch that L1 misses?\n",
    "    - Create figure: L1→L2→L3 progression\n",
    "    \n",
    "[ ] Write bullet points explaining WHY\n",
    "    - L1 (Baseline): \"Detect poles and signs\"\n",
    "    - L2 (Descriptive): \"Detect thin objects like poles, signs, fences\"\n",
    "    - Does extra detail help? By how much?\n",
    "\n",
    "PRIORITY 4 - Benchmark Against Baselines (2-3 days)\n",
    "─────────────────────────────────────────────────────────────────────────────\n",
    "[ ] Implement one strong baseline from paper:\n",
    "    Option A: Vanilla SegFormer (fastest to verify)\n",
    "    Option B: FCOS with boundary head (if code available)\n",
    "    Option C: Recent boundary detection from ICCV/CVPR\n",
    "    \n",
    "[ ] Fair comparison:\n",
    "    - Same training data/split\n",
    "    - Same inference setup (same images)\n",
    "    - Same metrics\n",
    "    \n",
    "[ ] Report: \"Our method is X% better than baseline\"\n",
    "\n",
    "KEY METRICS TO REPORT\n",
    "─────────────────────────────────────────────────────────────────────────────\n",
    "Must have for CVPR:\n",
    "✓ Overall accuracy (mIoU, Dice, F1)\n",
    "✓ Per-class breakdown (poles, signs, vegetation, etc.)\n",
    "✓ Baseline comparisons (SegFormer, Mask2Former, competitors)\n",
    "✓ Ablation table (L1 vs L2 vs L3 vs L4)\n",
    "✓ Computational cost (FPS, memory)\n",
    "✓ Statistical significance (confidence intervals, p-values)\n",
    "\n",
    "GENERATING THE PAPER\n",
    "─────────────────────────────────────────────────────────────────────────────\n",
    "Once you have:\n",
    "  1. Full evaluation results ✓\n",
    "  2. Baseline comparisons ✓\n",
    "  3. Ablation studies ✓\n",
    "  4. Prompt analysis ✓\n",
    "  \n",
    "You can write:\n",
    "  - Main results table (Table 1)\n",
    "  - Method comparison figure (Fig 3)\n",
    "  - Ablation table (Table 2)\n",
    "  - Example outputs (Fig 2, 4)\n",
    "  - Qualitative analysis\n",
    "\n",
    "ESTIMATED TIMELINE\n",
    "─────────────────────────────────────────────────────────────────────────────\n",
    "Week 1:     Full evaluation + baseline\n",
    "Week 2:     Prompt analysis + ablations\n",
    "Week 3:     Write paper draft\n",
    "Week 4:     Revisions + submission\n",
    "\n",
    "With this roadmap, you'll have a solid CVPR workshop submission ready in 3-4 weeks.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de3acc",
   "metadata": {},
   "source": [
    "## 6. Action Items Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0695f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║         RECOMMENDED NEXT STEPS FOR CVPR WORKSHOP PAPER QUALITY              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "PHASE 1: BASELINE ESTABLISHMENT (2-3 weeks)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "[ ] 1. Full-scale Evaluation\n",
    "    - Generate masks for FULL validation set (500 images)\n",
    "    - Compare all 4 configs on complete dataset\n",
    "    - Compute per-city performance breakdown\n",
    "    - Identify best-performing configuration\n",
    "\n",
    "[ ] 2. Implement Baseline Methods\n",
    "    - Mask2Former for boundary segmentation\n",
    "    - Traditional edge detection (Canny + morphology)\n",
    "    - Recent methods: EdgeNet, DFF, FCOS boundary\n",
    "    - SegFormer without auxiliary boundary loss\n",
    "    \n",
    "    → Target: Show 5-10% improvement over all baselines\n",
    "\n",
    "[ ] 3. Statistical Rigor\n",
    "    - Train/Val/Test split (60/20/20)\n",
    "    - Report confidence intervals\n",
    "    - Cross-validation results\n",
    "    - Per-class metrics (poles, signs, fences, etc.)\n",
    "\n",
    "PHASE 2: NOVEL CONTRIBUTION (3-4 weeks)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "[ ] 4. Deepen Prompt Analysis\n",
    "    ✓ Current: L1-L4 empirical hierarchy\n",
    "    ✓ Needed: Why does this work? Theoretical insight\n",
    "    \n",
    "    Experiments:\n",
    "    - Isolate L1→L2 improvement: +X% on IoU\n",
    "    - Isolate L2→L3 improvement: +Y% on IoU\n",
    "    - What makes descriptive/physical/specific prompts better?\n",
    "    - Analyze attention maps: does prompt change focus?\n",
    "\n",
    "[ ] 5. Prompt Learning (Novel)\n",
    "    Instead of fixed prompts → Learn prompts from data!\n",
    "    - Fine-tune prompt embeddings on boundary dataset\n",
    "    - Compare learned vs handcrafted prompts\n",
    "    - Potential publication angle: \"Learning Better Prompts for SAM\"\n",
    "\n",
    "[ ] 6. Adaptive Strategy Selection\n",
    "    - Train model to choose: baseline vs multi-crop vs tiled\n",
    "    - Input features: image stats, object density\n",
    "    - Adaptive routing could improve robustness\n",
    "    \n",
    "PHASE 3: COMPREHENSIVE EXPERIMENTS (2-3 weeks)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "[ ] 7. Ablation Studies\n",
    "    - Prompt level ablation (L1→2→3→4)\n",
    "    - Strategy ablation (baseline vs multi-crop vs tiled)\n",
    "    - Parameter sensitivity (grid size, window size, stride)\n",
    "    - Loss weighting in SegFormer training\n",
    "    \n",
    "    → Generate ablation table (Table 1 in paper)\n",
    "\n",
    "[ ] 8. Failure Analysis\n",
    "    - Where do methods fail? (small objects, shadows, clutter)\n",
    "    - Confusion matrix by object type\n",
    "    - Performance on hard negatives\n",
    "    - Examples: success vs failure cases\n",
    "    \n",
    "    → Generate failure case visualizations (Fig 4-5)\n",
    "\n",
    "[ ] 9. Computational Efficiency\n",
    "    - Inference time per image (baseline, multi-crop, tiled)\n",
    "    - GPU memory requirements\n",
    "    - Comparison to SegFormer baseline\n",
    "    - Trade-off: accuracy vs speed\n",
    "    \n",
    "    → Generate efficiency table\n",
    "\n",
    "[ ] 10. Generalization Testing\n",
    "    - Test on different urban datasets (if available)\n",
    "    - KITTI, Mapillary Vistas, or other urban data\n",
    "    - Cross-dataset evaluation\n",
    "    - Domain adaptation discussion\n",
    "\n",
    "PHASE 4: PAPER WRITING & POLISH (2 weeks)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "[ ] 11. Write Paper Structure\n",
    "    Abstract  → Motivate boundary problem + solution\n",
    "    Intro     → Why SAM+prompts? What's novel?\n",
    "    Method    → L1-L4 hierarchy + strategies + learning\n",
    "    Experiments → Results, ablations, analysis\n",
    "    Discussion → When/why it works, limitations\n",
    "    \n",
    "[ ] 12. Generate Figures\n",
    "    Fig 1: Overview of approach (prompts × strategies)\n",
    "    Fig 2: Example outputs across prompt levels\n",
    "    Fig 3: Method comparison (method × prompt heatmap)\n",
    "    Fig 4: Failure cases with analysis\n",
    "    Fig 5: Qualitative vs baselines\n",
    "    \n",
    "[ ] 13. Generate Tables\n",
    "    Table 1: Main results (method, metric, baseline comparison)\n",
    "    Table 2: Ablation study results\n",
    "    Table 3: Computational efficiency\n",
    "    Table 4: Per-class performance breakdown\n",
    "    \n",
    "[ ] 14. Polish & Submission\n",
    "    Proofread, check references, verify reproducibility\n",
    "    Release code + pretrained models (if possible)\n",
    "    Write supplementary material (more experiments)\n",
    "\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                         TIMELINE ESTIMATE: 8-12 WEEKS                        ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "Week 1-2:   Full evaluation + baseline implementation\n",
    "Week 3-4:   Prompt analysis + prompt learning experiments\n",
    "Week 5-6:   Ablation studies + efficiency analysis\n",
    "Week 7-8:   Failure analysis + generalization testing\n",
    "Week 9-10:  Paper writing + figure generation\n",
    "Week 11-12: Polish + submission ready\n",
    "\n",
    "QUICK WINS (Do First!)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "✓ Run full validation set (easiest, high impact)\n",
    "✓ Compare to one strong baseline (SegFormer alone)\n",
    "✓ Create ablation table (shows thoughtfulness)\n",
    "✓ Analyze L1→L4 progression (understand your method)\n",
    "\n",
    "These 4 items will immediately strengthen your positioning for CVPR.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618dfacc",
   "metadata": {},
   "source": [
    "### Current Strengths\n",
    "- **Prompt-based guidance**: L1-L4 hierarchy offers interpretable progression\n",
    "- **Multi-scale strategies**: Baseline, multi-crop, and tiled approaches address different failure modes\n",
    "- **SAM3 foundation**: Leverages powerful zero-shot segmentation model\n",
    "- **Boundary focus**: Addresses real urban problems (poles, signs, fences)\n",
    "\n",
    "### Key Challenges to Address for CVPR Quality\n",
    "\n",
    "**1. PERFORMANCE BASELINE**\n",
    "   - Need full-scale evaluation (not just 5 images)\n",
    "   - Comparative baselines: Mask2Former, EdgeNet, other boundary methods\n",
    "   - Statistical significance with proper train/val/test splits\n",
    "   - Must show 5-10% improvement over baselines\n",
    "\n",
    "**2. NOVEL CONTRIBUTION**\n",
    "   - Current: Prompt-based SAM3 variants (incremental)\n",
    "   - Needed: Unique insight on why prompts help boundaries\n",
    "   - Consider: Intermediate supervision, boundary-aware loss, or prompt learning\n",
    "\n",
    "**3. METHODOLOGY CLARITY**\n",
    "   - Formalize the prompt hierarchy (L1-L4)\n",
    "   - Mathematical framework for multi-crop/tiling strategies\n",
    "   - Clear algorithm descriptions\n",
    "\n",
    "**4. EXPERIMENTS & ABLATIONS**\n",
    "   - Ablation: Which prompt levels matter most?\n",
    "   - Ablation: Multi-crop vs tiling trade-offs\n",
    "   - Ablation: Effect of different thresholds/parameters\n",
    "   - Cross-dataset generalization (try on other urban datasets)\n",
    "\n",
    "**5. ANALYSIS & INSIGHTS**\n",
    "   - Failure case analysis (when/why do methods fail?)\n",
    "   - Computational efficiency comparison\n",
    "   - Qualitative analysis of prompt impact\n",
    "   - Per-class performance breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a2ffe",
   "metadata": {},
   "source": [
    "## 5. CVPR Workshop Paper Roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(image_idx=0, figsize=(18, 10)):\n",
    "    \"\"\"Visualize predictions across all configs for a single image\"\"\"\n",
    "    if len(results_df) == 0:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Get a sample image\n",
    "    sample = results_df.iloc[image_idx]\n",
    "    img_path = Path(sample['img_path'])\n",
    "    gt_path = Path(sample['gt_path'])\n",
    "    \n",
    "    if not img_path.exists() or not gt_path.exists():\n",
    "        print(f\"Image files not found for index {image_idx}\")\n",
    "        return\n",
    "    \n",
    "    # Load original image and GT\n",
    "    img = np.array(Image.open(img_path))\n",
    "    gt_boundary = extract_boundary_from_gt(gt_path)\n",
    "    \n",
    "    # Get all predictions for this image\n",
    "    same_image = results_df[results_df['image_id'] == sample['image_id']]\n",
    "    configs = same_image['config'].unique()\n",
    "    \n",
    "    n_configs = len(configs)\n",
    "    fig, axes = plt.subplots(2, n_configs + 1, figsize=figsize)\n",
    "    fig.suptitle(f\"Boundary Predictions Comparison: {sample['image_id']}\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(img)\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(gt_boundary, cmap='RdYlGn')\n",
    "    axes[1, 0].set_title(\"Ground Truth\")\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Predictions from each config\n",
    "    for col, config in enumerate(sorted(configs), 1):\n",
    "        row_data = same_image[same_image['config'] == config].iloc[0]\n",
    "        mask_path = Path(row_data['mask_path'])\n",
    "        \n",
    "        if mask_path.exists():\n",
    "            pred = np.load(mask_path).astype(bool)\n",
    "            iou = row_data['IoU']\n",
    "            dice = row_data['Dice']\n",
    "            \n",
    "            # Top row: prediction\n",
    "            axes[0, col].imshow(pred, cmap='RdYlGn')\n",
    "            axes[0, col].set_title(f\"{config[:30]}\\nIoU: {iou:.3f}\")\n",
    "            axes[0, col].axis('off')\n",
    "            \n",
    "            # Bottom row: overlay\n",
    "            overlay = np.zeros((*pred.shape, 3), dtype=np.uint8)\n",
    "            overlay[gt_boundary & pred] = [0, 255, 0]  # TP: green\n",
    "            overlay[gt_boundary & ~pred] = [255, 0, 0]  # FN: red\n",
    "            overlay[~gt_boundary & pred] = [0, 0, 255]  # FP: blue\n",
    "            \n",
    "            axes[1, col].imshow(overlay)\n",
    "            axes[1, col].set_title(f\"Dice: {dice:.3f}\")\n",
    "            axes[1, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT / f\"comparison_image_{image_idx}.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved comparison for {sample['image_id']}\")\n",
    "\n",
    "# Visualize first 3 images\n",
    "for i in range(min(3, len(results_df) // 4)):  # One per config per image\n",
    "    visualize_comparison(image_idx=i*4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819f377",
   "metadata": {},
   "source": [
    "## 4. Visual Comparison: Side-by-Side Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS BY METHOD\")\n",
    "    print(\"=\"*80)\n",
    "    method_stats = results_df.groupby(\"method\")[[\"IoU\", \"Dice\", \"F1\", \"Precision\", \"Recall\"]].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(method_stats.round(4))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS BY PROMPT\")\n",
    "    print(\"=\"*80)\n",
    "    prompt_stats = results_df.groupby(\"prompt\")[[\"IoU\", \"Dice\", \"F1\", \"Precision\", \"Recall\"]].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(prompt_stats.round(4))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS BY CONFIG\")\n",
    "    print(\"=\"*80)\n",
    "    config_stats = results_df.groupby(\"config\")[[\"IoU\", \"Dice\", \"F1\"]].agg(['mean', 'std', 'count'])\n",
    "    config_stats = config_stats.sort_values(('IoU', 'mean'), ascending=False)\n",
    "    print(config_stats.round(4))\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # T-test: baseline vs multi_crop\n",
    "    baseline_iou = results_df[results_df[\"method\"] == \"baseline\"][\"IoU\"].values\n",
    "    multicrop_iou = results_df[results_df[\"method\"] == \"multi_crop\"][\"IoU\"].values\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(baseline_iou, multicrop_iou)\n",
    "    print(f\"\\nMethod Comparison (Baseline vs Multi-Crop):\")\n",
    "    print(f\"  Baseline mean IoU:    {baseline_iou.mean():.4f} ± {baseline_iou.std():.4f}\")\n",
    "    print(f\"  Multi-Crop mean IoU:  {multicrop_iou.mean():.4f} ± {multicrop_iou.std():.4f}\")\n",
    "    print(f\"  T-test: t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"  ✓ SIGNIFICANT difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ✗ No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    # T-test: L1 vs L2\n",
    "    l1_iou = results_df[results_df[\"prompt\"] == \"Baseline\"][\"IoU\"].values\n",
    "    l2_iou = results_df[results_df[\"prompt\"] == \"Descriptive\"][\"IoU\"].values\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(l1_iou, l2_iou)\n",
    "    print(f\"\\nPrompt Level Comparison (L1 vs L2):\")\n",
    "    print(f\"  L1 (Baseline) mean IoU:     {l1_iou.mean():.4f} ± {l1_iou.std():.4f}\")\n",
    "    print(f\"  L2 (Descriptive) mean IoU:  {l2_iou.mean():.4f} ± {l2_iou.std():.4f}\")\n",
    "    print(f\"  T-test: t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"  ✓ SIGNIFICANT difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ✗ No significant difference (p >= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db75fac",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # 3. Heatmap: Method vs Prompt\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    pivot = results_df.pivot_table(values=\"IoU\", index=\"method\", columns=\"prompt\", aggfunc=\"mean\")\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt=\".4f\", cmap=\"RdYlGn\", ax=ax, cbar_kws={\"label\": \"IoU\"})\n",
    "    ax.set_title(\"IoU: Method × Prompt Level\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT / \"method_prompt_heatmap.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: method_prompt_heatmap.png\")\n",
    "    \n",
    "    # 4. Config rankings\n",
    "    config_summary = results_df.groupby(\"config\").agg({\n",
    "        \"IoU\": [\"mean\", \"std\"],\n",
    "        \"Dice\": \"mean\",\n",
    "        \"F1\": \"mean\",\n",
    "        \"Precision\": \"mean\",\n",
    "        \"Recall\": \"mean\"\n",
    "    }).round(4)\n",
    "    config_summary.columns = [\"IoU_mean\", \"IoU_std\", \"Dice\", \"F1\", \"Precision\", \"Recall\"]\n",
    "    config_summary = config_summary.sort_values(\"IoU_mean\", ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    configs = config_summary.index.tolist()\n",
    "    y_pos = np.arange(len(configs))\n",
    "    \n",
    "    ax.barh(y_pos, config_summary[\"IoU_mean\"].values, color=\"steelblue\", xerr=config_summary[\"IoU_std\"].values)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([c[:60] for c in configs], fontsize=10)\n",
    "    ax.set_xlabel(\"IoU Score\", fontweight='bold')\n",
    "    ax.set_title(\"Configuration Rankings by IoU\", fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT / \"config_rankings.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: config_rankings.png\")\n",
    "else:\n",
    "    print(\"⚠ No results to visualize\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def61169",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # 1. Performance by Method\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(\"Performance by Method (Baseline vs Multi-Crop)\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    metrics = [\"IoU\", \"Dice\", \"F1\"]\n",
    "    methods = sorted(results_df[\"method\"].unique())\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        data = [results_df[results_df[\"method\"] == m][metric].values for m in methods]\n",
    "        bp = axes[idx].boxplot(data, labels=methods, patch_artist=True)\n",
    "        \n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "        \n",
    "        axes[idx].set_ylabel(metric, fontweight='bold')\n",
    "        axes[idx].set_title(metric)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add mean values as text\n",
    "        for i, m in enumerate(methods):\n",
    "            mean_val = results_df[results_df[\"method\"] == m][metric].mean()\n",
    "            axes[idx].text(i+1, mean_val, f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT / \"method_performance.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: method_performance.png\")\n",
    "    \n",
    "    # 2. Performance by Prompt Level\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(\"Performance by Prompt Level (L1 vs L2)\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    prompts = sorted(results_df[\"prompt\"].unique())\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        data = [results_df[results_df[\"prompt\"] == p][metric].values for p in prompts]\n",
    "        bp = axes[idx].boxplot(data, labels=prompts, patch_artist=True)\n",
    "        \n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightcoral')\n",
    "        \n",
    "        axes[idx].set_ylabel(metric, fontweight='bold')\n",
    "        axes[idx].set_title(metric)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        for i, p in enumerate(prompts):\n",
    "            mean_val = results_df[results_df[\"prompt\"] == p][metric].mean()\n",
    "            axes[idx].text(i+1, mean_val, f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT / \"prompt_performance.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: prompt_performance.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b1c9c",
   "metadata": {},
   "source": [
    "## 2. Visualize Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1163f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all configurations against ground truth...\n",
      "✓ Evaluated 0 image-config pairs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['config', 'method', 'prompt', 'city', 'IoU', 'Dice', 'F1'], dtype='str')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Evaluated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m image-config pairs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmethod\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIoU\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mF1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/seg/lib/python3.13/site-packages/pandas/core/frame.py:4384\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4383\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4384\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4386\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/seg/lib/python3.13/site-packages/pandas/core/indexes/base.py:6302\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6300\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6302\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6304\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6306\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/seg/lib/python3.13/site-packages/pandas/core/indexes/base.py:6352\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6352\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6354\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['config', 'method', 'prompt', 'city', 'IoU', 'Dice', 'F1'], dtype='str')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "def evaluate_all_configs():\n",
    "    \"\"\"Evaluate all configs against ground truth\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Map image ID to GT path\n",
    "    gt_dir = DATA_ROOT / \"gtFine_trainvaltest\" / \"gtFine\" / \"val\"\n",
    "    img_dir = DATA_ROOT / \"leftImg8bit_trainvaltest\" / \"leftImg8bit\" / \"val\"\n",
    "    \n",
    "    for config_dir in sorted(RESULTS_ROOT.glob(\"sam3_*\")):\n",
    "        if not config_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        config_name = config_dir.name\n",
    "        # Extract method and prompt from name\n",
    "        parts = config_name.split(\"_\")\n",
    "        method = parts[1]  # baseline or multi_crop\n",
    "        prompt = \"_\".join(parts[2:]).split(\"_grid_size\")[0] if \"grid\" in config_name else \"_\".join(parts[2:])\n",
    "        \n",
    "        val_dir = config_dir / \"val\"\n",
    "        if not val_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        # Process each city/image\n",
    "        for city_dir in val_dir.iterdir():\n",
    "            if not city_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            for mask_file in city_dir.glob(\"*.npy\"):\n",
    "                image_id = mask_file.stem.replace(\"_boundary\", \"\")\n",
    "                city = city_dir.name\n",
    "                \n",
    "                # Find GT\n",
    "                gt_path = gt_dir / city / f\"{image_id}_gtFine_labelIds.png\"\n",
    "                img_path = img_dir / city / f\"{image_id}_leftImg8bit.png\"\n",
    "                \n",
    "                if not gt_path.exists() or not img_path.exists():\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Load masks\n",
    "                    pred_boundary = np.load(mask_file)\n",
    "                    gt_boundary = extract_boundary_from_gt(gt_path)\n",
    "                    \n",
    "                    # Ensure same shape\n",
    "                    if pred_boundary.shape != gt_boundary.shape:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute metrics\n",
    "                    metrics = compute_metrics(pred_boundary, gt_boundary)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"config\": config_name,\n",
    "                        \"method\": method,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"city\": city,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"mask_path\": str(mask_file),\n",
    "                        \"gt_path\": str(gt_path),\n",
    "                        \"img_path\": str(img_path),\n",
    "                        **metrics\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {mask_file}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Evaluating all configurations against ground truth...\")\n",
    "results_df = evaluate_all_configs()\n",
    "print(f\"✓ Evaluated {len(results_df)} image-config pairs\")\n",
    "print()\n",
    "print(results_df[[\"config\", \"method\", \"prompt\", \"city\", \"IoU\", \"Dice\", \"F1\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6a8819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST BATCH METADATA\n",
      "================================================================================\n",
      "Timestamp: 2026-02-02T08:17:54.066526\n",
      "Total Configs: 4\n",
      "Max Images per Config: 5\n",
      "Completed: 4/4\n",
      "Failed: 0\n",
      "\n",
      "Configurations:\n",
      "  ✓ baseline_L1_Baseline                               - 5 images\n",
      "  ✓ multi_crop_L1_Baseline_grid_size=(2, 2)            - 5 images\n",
      "  ✓ baseline_L2_Descriptive                            - 5 images\n",
      "  ✓ multi_crop_L2_Descriptive_grid_size=(2, 2)         - 5 images\n"
     ]
    }
   ],
   "source": [
    "def load_metadata():\n",
    "    \"\"\"Load generation metadata\"\"\"\n",
    "    meta_path = RESULTS_ROOT / \"generation_metadata.json\"\n",
    "    with open(meta_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_boundary_from_gt(gt_path):\n",
    "    \"\"\"Extract boundary from Cityscapes ground truth\"\"\"\n",
    "    labels = np.array(Image.open(gt_path), dtype=np.uint8)\n",
    "    # Thin object classes: poles, traffic signs, vegetation, building, walls\n",
    "    thin_classes = [4, 5, 6, 7, 11, 12, 17, 18]\n",
    "    \n",
    "    thin_mask = np.zeros_like(labels, dtype=bool)\n",
    "    for cls in thin_classes:\n",
    "        thin_mask |= (labels == cls)\n",
    "    \n",
    "    if thin_mask.sum() == 0:\n",
    "        return np.zeros_like(thin_mask, dtype=bool)\n",
    "    \n",
    "    # Extract edges using Canny\n",
    "    edges = cv2.Canny((thin_mask * 255).astype(np.uint8), 100, 200)\n",
    "    boundary = cv2.dilate(edges.astype(np.uint8), np.ones((3,3), np.uint8), iterations=1).astype(bool)\n",
    "    return boundary\n",
    "\n",
    "def compute_metrics(pred, gt):\n",
    "    \"\"\"Compute boundary detection metrics\"\"\"\n",
    "    pred = pred.astype(bool)\n",
    "    gt = gt.astype(bool)\n",
    "    \n",
    "    intersection = (pred & gt).sum()\n",
    "    union = (pred | gt).sum()\n",
    "    iou = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    tp = intersection\n",
    "    fp = (pred & ~gt).sum()\n",
    "    fn = (~pred & gt).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    dice = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"IoU\": iou,\n",
    "        \"Dice\": dice,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"TP\": int(tp),\n",
    "        \"FP\": int(fp),\n",
    "        \"FN\": int(fn)\n",
    "    }\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_metadata()\n",
    "print(\"=\"*80)\n",
    "print(\"TEST BATCH METADATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Timestamp: {metadata['timestamp']}\")\n",
    "print(f\"Total Configs: {metadata['total_configs']}\")\n",
    "print(f\"Max Images per Config: {metadata['max_images']}\")\n",
    "print(f\"Completed: {metadata['completed']}/{metadata['total_configs']}\")\n",
    "print(f\"Failed: {metadata['failed']}\")\n",
    "print()\n",
    "print(\"Configurations:\")\n",
    "for cfg in metadata['configs']:\n",
    "    print(f\"  ✓ {cfg['config']:50} - {cfg['images_processed']} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77561abf",
   "metadata": {},
   "source": [
    "## 1. Load and Parse Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b216eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n",
      "  Results: ../outputs/test_batch_masks\n",
      "  Data: ../data/cityscapes\n",
      "  Analysis Output: ../outputs/test_batch_masks/analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "RESULTS_ROOT = Path(\"../outputs/test_batch_masks\")\n",
    "DATA_ROOT = Path(\"../data/cityscapes\")\n",
    "ANALYSIS_OUTPUT = RESULTS_ROOT / \"analysis\"\n",
    "ANALYSIS_OUTPUT.mkdir(exist_ok=True)\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"✓ Environment configured\")\n",
    "print(f\"  Results: {RESULTS_ROOT}\")\n",
    "print(f\"  Data: {DATA_ROOT}\")\n",
    "print(f\"  Analysis Output: {ANALYSIS_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c5ab6",
   "metadata": {},
   "source": [
    "# Batch Test Analysis & CVPR Paper Roadmap\n",
    "\n",
    "**Objective**: Analyze the batch test results from SAM3 boundary generation, evaluate performance metrics, visualize comparisons, and develop a strategic roadmap for CVPR workshop paper-quality research.\n",
    "\n",
    "**Test Config**: 4 configurations × 5 images = 20 results\n",
    "- baseline_L1_Baseline\n",
    "- baseline_L2_Descriptive  \n",
    "- multi_crop_L1_Baseline (2×2 grid)\n",
    "- multi_crop_L2_Descriptive (2×2 grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
