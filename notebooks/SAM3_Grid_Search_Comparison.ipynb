{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dbe2971",
   "metadata": {},
   "source": [
    "# SAM3 Grid Search Quick Analysis\n",
    "Efficient analysis of SAM3 boundary generation configurations with ground truth comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d753fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Paths configured\n",
      "  Data: ../data/cityscapes\n",
      "  Results: ../grid_search_results/analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOT = Path(\"../data/cityscapes\")\n",
    "GRID_SEARCH_ROOT = Path(\"../grid_search_results\")\n",
    "RESULTS_DIR = GRID_SEARCH_ROOT / \"analysis\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"âœ“ Paths configured\")\n",
    "print(f\"  Data: {DATA_ROOT}\")\n",
    "print(f\"  Results: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66045693",
   "metadata": {},
   "source": [
    "## 1. Discover and Load Configuration Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_configs():\n",
    "    \"\"\"Find all generated configurations quickly\"\"\"\n",
    "    masks_dir = GRID_SEARCH_ROOT / \"masks\"\n",
    "    if not masks_dir.exists():\n",
    "        return {}\n",
    "    \n",
    "    configs = {}\n",
    "    \n",
    "    # Look for any subdirectories containing val/ folders with .npy files\n",
    "    for root, dirs, files in os.walk(masks_dir):\n",
    "        if \"val\" in dirs:\n",
    "            val_path = Path(root) / \"val\"\n",
    "            npy_files = list(val_path.glob(\"*/*.npy\"))\n",
    "            \n",
    "            if npy_files:\n",
    "                # Extract config info from path\n",
    "                config_dir_name = Path(root).name\n",
    "                if config_dir_name.startswith(\"sam3_boundary_\"):\n",
    "                    name = config_dir_name.replace(\"sam3_boundary_\", \"\")\n",
    "                    parts = name.split(\"_\")\n",
    "                    \n",
    "                    method = parts[0] if len(parts) > 0 else \"unknown\"\n",
    "                    prompt = parts[1] if len(parts) > 1 else \"unknown\"\n",
    "                    \n",
    "                    configs[name] = {\n",
    "                        \"path\": Path(root),\n",
    "                        \"method\": method,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"num_files\": len(npy_files)\n",
    "                    }\n",
    "    \n",
    "    return configs\n",
    "\n",
    "configs = discover_configs()\n",
    "print(f\"âœ“ Found {len(configs)} configurations with generated masks\")\n",
    "if configs:\n",
    "    print(\"\\nFound configurations:\")\n",
    "    for name, info in list(configs.items())[:5]:\n",
    "        print(f\"  {info['method']:12} {info['prompt']:16} - {info['num_files']:3} masks\")\n",
    "else:\n",
    "    print(\"âš  No masks found yet\")\n",
    "    print(\"Run: python grid_search_sam3_generation.py --data_root ./data/cityscapes\")\n",
    "    print(\"Or:  python quick_launch_sam3.py balanced --data_root ./data/cityscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557577e3",
   "metadata": {},
   "source": [
    "## 2. Load Validation Images and Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704303c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_images():\n",
    "    \"\"\"Quick mapping of validation images to ground truth\"\"\"\n",
    "    images_dir = DATA_ROOT / \"leftImg8bit_trainvaltest\" / \"leftImg8bit\" / \"val\"\n",
    "    gt_dir = DATA_ROOT / \"gtFine_trainvaltest\" / \"gtFine\" / \"val\"\n",
    "    \n",
    "    val_images = {}\n",
    "    for img_path in glob.glob(str(images_dir / \"*\" / \"*_leftImg8bit.png\")):\n",
    "        img_path = Path(img_path)\n",
    "        city = img_path.parent.name\n",
    "        filename = img_path.stem.replace(\"_leftImg8bit\", \"\")\n",
    "        \n",
    "        gt_path = gt_dir / city / f\"{filename}_gtFine_labelIds.png\"\n",
    "        if gt_path.exists():\n",
    "            image_id = f\"{city}_{filename}\"\n",
    "            val_images[image_id] = {\"gt\": gt_path}\n",
    "    \n",
    "    return val_images\n",
    "\n",
    "def extract_boundary_from_gt(gt_path):\n",
    "    \"\"\"Fast boundary extraction from labels\"\"\"\n",
    "    labels = np.array(Image.open(gt_path), dtype=np.uint8)\n",
    "    thin_classes = [4, 5, 6, 7, 11, 12, 17, 18]\n",
    "    \n",
    "    thin_mask = np.zeros_like(labels, dtype=bool)\n",
    "    for cls in thin_classes:\n",
    "        thin_mask |= (labels == cls)\n",
    "    \n",
    "    if thin_mask.sum() == 0:\n",
    "        return np.zeros_like(thin_mask, dtype=bool)\n",
    "    \n",
    "    edges = cv2.Canny((thin_mask * 255).astype(np.uint8), 100, 200)\n",
    "    boundary = cv2.dilate(edges.astype(np.uint8), np.ones((3,3), np.uint8), iterations=1).astype(bool)\n",
    "    return boundary\n",
    "\n",
    "val_images = get_validation_images()\n",
    "print(f\"âœ“ Found {len(val_images)} validation images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b111aa",
   "metadata": {},
   "source": [
    "## 3. Compute Metrics for All Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136851be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, gt):\n",
    "    \"\"\"Fast metric computation\"\"\"\n",
    "    pred = pred.astype(bool)\n",
    "    gt = gt.astype(bool)\n",
    "    \n",
    "    intersection = (pred & gt).sum()\n",
    "    union = (pred | gt).sum()\n",
    "    iou = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    tp = intersection\n",
    "    fp = (pred & ~gt).sum()\n",
    "    fn = (~pred & gt).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    dice = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n",
    "    \n",
    "    return {\"IoU\": iou, \"Dice\": dice, \"Precision\": precision, \"Recall\": recall, \"F1\": f1}\n",
    "\n",
    "def evaluate_all_configs():\n",
    "    \"\"\"Efficiently evaluate all configurations\"\"\"\n",
    "    if not configs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    for config_name, config_info in configs.items():\n",
    "        config_dir = config_info[\"path\"]\n",
    "        val_dir = config_dir / \"val\"\n",
    "        \n",
    "        if not val_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        for image_id, img_info in val_images.items():\n",
    "            if \"gt\" not in img_info:\n",
    "                continue\n",
    "            \n",
    "            gt_path = img_info[\"gt\"]\n",
    "            city = image_id.split(\"_\")[0]\n",
    "            filename = \"_\".join(image_id.split(\"_\")[1:])\n",
    "            \n",
    "            mask_path = val_dir / city / f\"{filename}.npy\"\n",
    "            if not mask_path.exists():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                gt_boundary = extract_boundary_from_gt(gt_path)\n",
    "                pred_boundary = np.load(mask_path)\n",
    "                \n",
    "                # Ensure same shape\n",
    "                if pred_boundary.shape != gt_boundary.shape:\n",
    "                    continue\n",
    "                \n",
    "                metrics = compute_metrics(pred_boundary, gt_boundary)\n",
    "                results.append({\n",
    "                    \"config\": config_name,\n",
    "                    \"method\": config_info[\"method\"],\n",
    "                    \"prompt\": config_info[\"prompt\"],\n",
    "                    \"image_id\": image_id,\n",
    "                    **metrics\n",
    "                })\n",
    "                total_processed += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    print(f\"  Processed {total_processed} image-config pairs\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if configs:\n",
    "    print(\"Evaluating all configurations...\")\n",
    "    results_df = evaluate_all_configs()\n",
    "    print(f\"âœ“ Computed metrics for {len(results_df)} comparisons\")\n",
    "else:\n",
    "    results_df = pd.DataFrame()\n",
    "    print(\"No configurations to evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fea9d1",
   "metadata": {},
   "source": [
    "## 4. Create Summary Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # Configuration summary\n",
    "    config_summary = results_df.groupby([\"config\", \"method\", \"prompt\"]).agg({\n",
    "        \"IoU\": [\"mean\", \"std\", \"count\"],\n",
    "        \"Dice\": \"mean\",\n",
    "        \"F1\": \"mean\",\n",
    "        \"Precision\": \"mean\",\n",
    "        \"Recall\": \"mean\"\n",
    "    }).round(4).reset_index()\n",
    "    \n",
    "    config_summary.columns = [\"config\", \"method\", \"prompt\", \"IoU_mean\", \"IoU_std\", \"num_images\", \"Dice\", \"F1\", \"Precision\", \"Recall\"]\n",
    "    config_summary = config_summary.sort_values(\"IoU_mean\", ascending=False)\n",
    "    \n",
    "    # Method summary\n",
    "    method_summary = results_df.groupby(\"method\").agg({\n",
    "        \"IoU\": [\"mean\", \"std\", \"count\"],\n",
    "        \"Dice\": \"mean\",\n",
    "        \"F1\": \"mean\"\n",
    "    }).round(4)\n",
    "    \n",
    "    # Prompt summary\n",
    "    prompt_summary = results_df.groupby(\"prompt\").agg({\n",
    "        \"IoU\": [\"mean\", \"std\", \"count\"],\n",
    "        \"Dice\": \"mean\",\n",
    "        \"F1\": \"mean\"\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"TOP 10 CONFIGURATIONS BY IoU (from {len(config_summary)} total)\")\n",
    "    print(\"=\"*80)\n",
    "    print(config_summary.head(10)[[\"config\", \"method\", \"prompt\", \"num_images\", \"IoU_mean\", \"Dice\", \"F1\"]].to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"METHOD PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(method_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROMPT LEVEL PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(prompt_summary)\n",
    "    \n",
    "    # Save results\n",
    "    config_summary.to_csv(RESULTS_DIR / \"configuration_summary.csv\", index=False)\n",
    "    results_df.to_csv(RESULTS_DIR / \"detailed_results.csv\", index=False)\n",
    "    print(f\"\\nâœ“ Results saved:\")\n",
    "    print(f\"  {RESULTS_DIR / 'configuration_summary.csv'}\")\n",
    "    print(f\"  {RESULTS_DIR / 'detailed_results.csv'}\")\n",
    "else:\n",
    "    print(\"âš  No evaluation results yet\")\n",
    "    print(\"\\nTo generate results, run:\")\n",
    "    print(\"  python quick_launch_sam3.py balanced --data_root ./data/cityscapes\")\n",
    "    print(\"Then come back and run this notebook again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ff020",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # 1. Method performance\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle(\"Performance by Method\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    metrics = [\"IoU\", \"Dice\", \"F1\"]\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        data = [results_df[results_df[\"method\"] == m][metric].values for m in sorted(results_df[\"method\"].unique())]\n",
    "        bp = axes[idx].boxplot(data, labels=sorted(results_df[\"method\"].unique()), patch_artist=True)\n",
    "        \n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "        \n",
    "        axes[idx].set_ylabel(metric)\n",
    "        axes[idx].set_title(metric)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"method_performance.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ“ Saved: method_performance.png\")\n",
    "    \n",
    "    # 2. Heatmap: Method vs Prompt\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    pivot = results_df.pivot_table(values=\"IoU\", index=\"method\", columns=\"prompt\", aggfunc=\"mean\")\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"RdYlGn\", ax=ax, cbar_kws={\"label\": \"IoU\"})\n",
    "    ax.set_title(\"IoU: Method vs Prompt Level\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"method_prompt_heatmap.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ“ Saved: method_prompt_heatmap.png\")\n",
    "    \n",
    "    # 3. Top configs ranking\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    top_configs = config_summary.head(15)\n",
    "    \n",
    "    y_pos = np.arange(len(top_configs))\n",
    "    ax.barh(y_pos, top_configs[\"IoU_mean\"].values, color=\"steelblue\", xerr=top_configs[\"IoU_std\"].values)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([c[:50] for c in top_configs[\"config\"].values], fontsize=9)\n",
    "    ax.set_xlabel(\"IoU Score\", fontweight='bold')\n",
    "    ax.set_title(\"Top 15 Configurations\", fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"top_configurations.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ“ Saved: top_configurations.png\")\n",
    "else:\n",
    "    print(\"No data for visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fd474",
   "metadata": {},
   "source": [
    "## 6. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc33220",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS FOR BOUNDARY TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best overall\n",
    "    best = config_summary.iloc[0]\n",
    "    print(f\"\\nðŸ¥‡ OVERALL BEST\")\n",
    "    print(f\"  Config: {best['config']}\")\n",
    "    print(f\"  IoU: {best['IoU_mean']:.4f} Â± {best['IoU_std']:.4f}\")\n",
    "    print(f\"  Dice: {best['Dice']:.4f}\")\n",
    "    print(f\"  F1: {best['F1']:.4f}\")\n",
    "    \n",
    "    # Best per method\n",
    "    print(f\"\\nâš¡ BEST PER METHOD\")\n",
    "    for method in sorted(results_df[\"method\"].unique()):\n",
    "        method_best = config_summary[config_summary[\"method\"] == method].iloc[0]\n",
    "        print(f\"  {method:12}: {method_best['prompt']:16} (IoU: {method_best['IoU_mean']:.4f})\")\n",
    "    \n",
    "    # Best per prompt\n",
    "    print(f\"\\nðŸ’¬ BEST PER PROMPT LEVEL\")\n",
    "    for prompt in sorted(results_df[\"prompt\"].unique()):\n",
    "        prompt_best = config_summary[config_summary[\"prompt\"] == prompt].iloc[0]\n",
    "        print(f\"  {prompt:16}: {prompt_best['method']:12} (IoU: {prompt_best['IoU_mean']:.4f})\")\n",
    "    \n",
    "    # Export report\n",
    "    report = f\"\"\"\n",
    "SAM3 GRID SEARCH RESULTS\n",
    "Generated: {pd.Timestamp.now()}\n",
    "\n",
    "BEST CONFIGURATION\n",
    "{best['config']}\n",
    "- IoU: {best['IoU_mean']:.4f} Â± {best['IoU_std']:.4f}\n",
    "- Dice: {best['Dice']:.4f}\n",
    "- F1: {best['F1']:.4f}\n",
    "\n",
    "METHOD COMPARISON\n",
    "{method_summary.to_string()}\n",
    "\n",
    "PROMPT LEVEL COMPARISON  \n",
    "{prompt_summary.to_string()}\n",
    "\n",
    "KEY INSIGHTS\n",
    "- Total configurations tested: {len(config_summary)}\n",
    "- Total image evaluations: {len(results_df)}\n",
    "- Best performing method: {config_summary.iloc[0]['method']}\n",
    "- Best performing prompt: {config_summary.iloc[0]['prompt']}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(RESULTS_DIR / \"RECOMMENDATIONS.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"\\nâœ“ Full report saved to RECOMMENDATIONS.txt\")\n",
    "else:\n",
    "    print(\"No results to show recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
